{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9cceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "import seaborn as sb\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3218615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# A helper function for creating Confusion Matrix and displaying the report\n",
    "def Confusion_Matrix(y_true, y_pred, labels, fold_count):\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "                                                        \n",
    "    plt.figure(figsize = (10,5))\n",
    "    sb.set(font_scale=1.2)\n",
    "    sb.heatmap(conf_mat, annot=True, cmap=\"YlGnBu\", fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix for fold \" + str(fold_count))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Classification Report:\\n\\n\",classification_report(y_true, y_pred))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b889a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_twitter_data = pd.read_csv(\"dataset/clean_twitter_data.csv\", index_col = [0],encoding=\"utf-8\")\n",
    "clean_twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_twitter_data = pd.read_csv(\"dataset/clean_twitter_data.csv\", index_col = [0],encoding=\"utf-8\")\n",
    "clean_twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are tweets where after pre-processing are just empty strings\n",
    "clean_twitter_data.dropna(inplace=True)\n",
    "clean_twitter_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11deadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what is the longest string of tweet\n",
    "text_length =  clean_twitter_data.clean_tweets.str.len()\n",
    "max_len = max(text_length)\n",
    "max_len  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ceb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_SIZE = 300 # represents how big your dimensional size of your word2vec, \n",
    "W2V_WINDOW = 2 # Window_Size refers to the number of words on either side of a target word that are used to predict the target word\n",
    "              # so if size = 5 , the window is (n-5),(n-4), (n-3)....n, (n+1)....(n+5)word\n",
    "W2V_EPOCH = 32 # Number of epochs to train the word2vec\n",
    "W2V_MIN_COUNT = 2 # This mean that, if the word that occurs less than 2 times, will be drop away from the dictionary\n",
    "\n",
    "word2vec_model = Word2Vec(vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfe26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(text) for text in clean_twitter_data.clean_tweets]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23260f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.build_vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, x_test, Y, y_test = train_test_split(clean_twitter_data['clean_tweets'], clean_twitter_data['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(Y)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenization helps to reduce the dimensionality of the data, making it easier to analyze and process.\n",
    "tokenizer.fit_on_texts(clean_twitter_data.clean_tweets)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size) # Shows the vocacbulary size of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(doc, w2v_model, embedding_dim):\n",
    "    words = doc.split()\n",
    "    # filter out-of-vocabulary words\n",
    "    words = [word for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if len(words) > 0:\n",
    "        return w2v_model.wv.get_mean_vector(words)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac91a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embeddings = [get_word_embedding(doc, word2vec_model, embedding_dim=W2V_SIZE) for doc in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_history = {}\n",
    "# K-Fold validation\n",
    "num_classes = 3\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_count = 1\n",
    "\n",
    "rf_1 = None\n",
    "rf_2 = None\n",
    "rf_3 = None\n",
    "rf_4 = None\n",
    "rf_5 = None\n",
    "\n",
    "rf_list = [rf_1, rf_2, rf_3, rf_4, rf_5]\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    le = LabelEncoder()\n",
    "    X_train, X_val =X[\"clean_tweets\"].iloc[train_index],X[\"clean_tweets\"].iloc[val_index]\n",
    "    y_train, y_val = Y[\"class\"].iloc[train_index], Y[\"class\"].iloc[val_index]\n",
    "    \n",
    "    X_train_embeddings = [get_word_embedding(doc, word2vec_model, embedding_dim=W2V_SIZE) for doc in X_train]\n",
    "    X_val_embeddings = [get_word_embedding(doc, word2vec_model, embedding_dim=W2V_SIZE) for doc in X_val]\n",
    "    \n",
    "    print(y_train)\n",
    "    scaler_Word2Vec = MinMaxScaler(feature_range=(0,1))\n",
    "    X_train_embeddings = scaler_Word2Vec.fit_transform(X_train_embeddings)\n",
    "    X_val_embeddings = scaler_Word2Vec.transform(X_val_embeddings)\n",
    "    X_test_scaled = scaler_Word2Vec.transform(X_test_embeddings)\n",
    "    \n",
    "    # label encoding for our categorical variable\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_val = le.transform(y_val)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    # One-hot encode the target variable\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_val = to_categorical(y_val, num_classes)\n",
    "    y_test_encoded = to_categorical(y_test_encoded, num_classes)\n",
    "    \n",
    "    rf_list[fold_count-1] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    # fit the model on the training data\n",
    "    rf_list[fold_count-1].fit(X_train_embeddings, y_train)\n",
    "    \n",
    "    y_pred = rf_list[fold_count-1].predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "    \n",
    "    y_test_labels = np.argmax(y_test_encoded, axis=1)\n",
    "    y_test_labels = le.inverse_transform(y_test_labels)\n",
    "    Confusion_Matrix( y_test_labels, y_pred_labels, np.unique(clean_twitter_data[\"class\"]), fold_count)\n",
    "    fold_count+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techfest_env",
   "language": "python",
   "name": "techfest_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
